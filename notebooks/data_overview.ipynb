{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9814b4e",
   "metadata": {},
   "source": [
    "# WJEC Copyediting Results Analysis\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook analyses the results from the various automated copyediting tools applied to the entire corpus of WJEC 'Made for Wales' GCSE documents. These were scraped directly from the WJEC website on November 8th 2025. I pushed them to GitHub at this time to ensure that there is an immutable record of the exact documents used for this analysis.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Data Collection\n",
    "\n",
    "1. **Scraping**: The documents were scraped from the WJEC website using a custom Python script that navigated to the relevant pages and downloaded the PDF files.\n",
    "2. **Conversion**: The PDFs were converted to markdown using [Marker](https://github.com/datalab-to/marker), aided by `Gemini-2.5-Flash-Lite` to enchance text extraction quality. I also tried Docling and PDF2Markdown but marker produced the best results.\n",
    "\n",
    "### Copyediting Process\n",
    "\n",
    "Once processed, the proofreading tools were applied to markdown in passes, with each pass building on the previous one.\n",
    "\n",
    "#### 1. Language Tool Spelling and Grammar Checker\n",
    "\n",
    "This tool was used to prove that the WJEC had not even bothered to run a basic spelling and grammar check on their documents. It is a free and open-source spelling a grammar checker.\n",
    "\n",
    "Several passes were made,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8657a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and versions\n",
    "import sys, platform, subprocess\n",
    "import pandas as pd\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import duckdb\n",
    "\n",
    "print('Python:', sys.version.splitlines()[0])\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Plotly:', plotly.__version__)\n",
    "print('DuckDB:', duckdb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to key CSVs\n",
    "DATA_DOCS = 'document_stats.csv'\n",
    "DATA_FILES = 'document_stats-files.csv'\n",
    "DATA_LANG = 'Documents/language-check-report.csv'\n",
    "\n",
    "# Load (these are small enough to load into memory in this repo)\n",
    "docs = pd.read_csv(DATA_DOCS)\n",
    "files = pd.read_csv(DATA_FILES)\n",
    "lang = pd.read_csv(DATA_LANG)\n",
    "\n",
    "docs.shape, files.shape, lang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc8033f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick heads and basic stats\n",
    "display(docs.head())\n",
    "display(docs.describe(include='all'))\n",
    "\n",
    "display(files.head())\n",
    "display(files['Pages'].describe())\n",
    "\n",
    "display(lang.head())\n",
    "print('Language-check issues by type:')\n",
    "print(lang['Type'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fbe14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example plot: PDFs per subject (from document_stats.csv)\n",
    "fig = px.bar(docs, x='Subject', y='PDFs', title='PDFs per subject')\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'}, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c406ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of pages per file (document_stats-files.csv)\n",
    "fig = px.histogram(files, x='Pages', nbins=30, title='Distribution of pages per file')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffdd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join example: verify per-subject page totals\n",
    "files_by_subject = files.groupby('Subject', as_index=False)['Pages'].sum().rename(columns={'Pages':'Pages_files'})\n",
    "merged = pd.merge(docs, files_by_subject, on='Subject', how='left')\n",
    "merged['Pages_diff'] = merged['Pages'] - merged['Pages_files']\n",
    "display(merged[['Subject','Pages','Pages_files','Pages_diff']].sort_values('Pages_diff', key=abs, ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7741e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top subjects by language-check issues (quick summary)\n",
    "top_issues = lang.groupby('Subject').size().reset_index(name='n').sort_values('n', ascending=False).head(25)\n",
    "fig = px.bar(top_issues, x='Subject', y='n', title='Top subjects by language-check issues')\n",
    "fig.update_layout(xaxis={'categoryorder':'total descending'}, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f931eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a small processed summary for quick reference\n",
    "import os\n",
    "os.makedirs('notebooks', exist_ok=True)\n",
    "merged.to_csv('notebooks/processed_summary.csv', index=False)\n",
    "print('Wrote notebooks/processed_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3440df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility: git commit and environment\n",
    "try:\n",
    "    sha = subprocess.check_output(['git','rev-parse','--short','HEAD']).decode().strip()\n",
    "except Exception:\n",
    "    sha = '<not available>'\n",
    "print('Git commit:', sha)\n",
    "print('Platform:', platform.platform())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7d8696",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Add a DuckDB-powered cell for SQL analytics without fully loading very large CSVs.\n",
    "- Add more focused visualisations (issue types over time, heatmaps by rule ID, per-file issue density).\n",
    "- Optionally add `nbval` or `papermill` based CI to execute this notebook in CI and ensure it runs without errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
